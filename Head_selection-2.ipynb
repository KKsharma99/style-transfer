{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Head_selection-checkpoint.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0HxiCqNoHq7C",
        "outputId": "b9aee588-4bd6-4551-c441-f9b0ddab5440",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "! git clone https://github.com/tuanyuan2008/style-transfer"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'style-transfer'...\n",
            "remote: Enumerating objects: 413, done.\u001b[K\n",
            "remote: Counting objects: 100% (413/413), done.\u001b[K\n",
            "remote: Compressing objects: 100% (180/180), done.\u001b[K\n",
            "remote: Total 413 (delta 234), reused 402 (delta 227), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (413/413), 42.03 MiB | 25.54 MiB/s, done.\n",
            "Resolving deltas: 100% (234/234), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "i_x0G8hPH3iH",
        "outputId": "258c2c1e-a2d0-429a-a122-beccd0570e68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "% cd style-transfer"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/style-transfer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oFMb-uH_Hjrj"
      },
      "source": [
        "# Head Selection\n",
        "**_BERT_** is a **_Multi-layer_ _Multi-Head_** Transformer architecture. As discuss in many of the current reseachers, different Attention heads captures different lingustic patterns. For a better deletion of words using Attention mechanism we need to choose a head which **captures pattern useful for classification.**\n",
        "\n",
        "To do this we are using a Brute force mechanism to seach through all the possible heads. We are deleting TopK words attended by different heads from the sentence and measuring the new classification score. In case of sentiments, removing sentiments related words makes the sentence neutral. The heads are sorted by the amount to which it is able to make the sentences from dev set to Neutral."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZbYvDRBDJNVz",
        "outputId": "cf7d76a9-5820-4dac-b336-bcb8c8d80db3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "! sudo apt-get install git-lfs\n",
        "! git-lfs pull"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  git-lfs\n",
            "0 upgraded, 1 newly installed, 0 to remove and 25 not upgraded.\n",
            "Need to get 2,129 kB of archives.\n",
            "After this operation, 7,662 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 git-lfs amd64 2.3.4-1 [2,129 kB]\n",
            "Fetched 2,129 kB in 1s (2,014 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package git-lfs.\n",
            "(Reading database ... 144568 files and directories currently installed.)\n",
            "Preparing to unpack .../git-lfs_2.3.4-1_amd64.deb ...\n",
            "Unpacking git-lfs (2.3.4-1) ...\n",
            "Setting up git-lfs (2.3.4-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Git LFS: (1 of 1 files) 417.69 MB / 417.69 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sTlmdxkRHjrk",
        "outputId": "758cf64d-39c2-43c9-fecf-71b5f5dd54f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import csv\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
        "                              TensorDataset)\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE\n",
        "from pytorch_pretrained_bert.modeling import BertForSequenceClassification, BertConfig, WEIGHTS_NAME, CONFIG_NAME\n",
        "#from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
        "from pytorch_pretrained_bert.optimization import BertAdam, warmup_linear\n",
        "\n",
        "from bertviz.bertviz import attention, visualization\n",
        "from bertviz.bertviz.pytorch_pretrained_bert import BertModel, BertTokenizer\n",
        "\n",
        "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
        "                    level = logging.INFO)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
            "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hssG7C6mHrQ8"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ILTSuYGPHjrq",
        "outputId": "c60da793-3976-4fdd-b242-90b4c74ba156",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "logger = logging.getLogger(__name__)\n",
        "bert_classifier_model_dir = \"models/\" ## Path of BERT classifier model path\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "logger.info(\"device: {}, n_gpu {}\".format(device, n_gpu))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "04/23/2020 23:44:25 - INFO - __main__ -   device: cuda, n_gpu 1\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "owxe6-33Hjru",
        "outputId": "4acee8b0-3720-4b57-f4e3-11f8d7ca0049",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## Model for performing Classification\n",
        "model_cls = BertForSequenceClassification.from_pretrained(bert_classifier_model_dir, num_labels=2)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "model_cls.to(device)\n",
        "model_cls.eval()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "04/23/2020 23:48:22 - INFO - pytorch_pretrained_bert.modeling -   loading archive file models/\n",
            "04/23/2020 23:48:22 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "04/23/2020 23:48:33 - INFO - bertviz.bertviz.pytorch_pretrained_bert.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache, downloading to /tmp/tmp13_wb8vv\n",
            "100%|██████████| 231508/231508 [00:00<00:00, 2561781.06B/s]\n",
            "04/23/2020 23:48:33 - INFO - bertviz.bertviz.pytorch_pretrained_bert.file_utils -   copying /tmp/tmp13_wb8vv to cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "04/23/2020 23:48:33 - INFO - bertviz.bertviz.pytorch_pretrained_bert.file_utils -   creating metadata file for /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "04/23/2020 23:48:33 - INFO - bertviz.bertviz.pytorch_pretrained_bert.file_utils -   removing temp file /tmp/tmp13_wb8vv\n",
            "04/23/2020 23:48:33 - INFO - bertviz.bertviz.pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lOIDz8YpHjrx",
        "outputId": "df79b9da-782e-4bed-e58b-3e63eb13c367",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## Model to get the attention weights of all the heads\n",
        "model = BertModel.from_pretrained(bert_classifier_model_dir)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "model.to(device)\n",
        "model.eval()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "04/23/2020 23:48:40 - INFO - bertviz.bertviz.pytorch_pretrained_bert.modeling -   loading archive file models/\n",
            "04/23/2020 23:48:40 - INFO - bertviz.bertviz.pytorch_pretrained_bert.modeling -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "04/23/2020 23:48:42 - INFO - bertviz.bertviz.pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): BertLayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ihDEtkAEHjr0",
        "colab": {}
      },
      "source": [
        "max_seq_len=512 # Maximum sequence length \n",
        "sm = torch.nn.Softmax(dim=-1) ## Softmax over the batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9gelEGpYHjr2",
        "colab": {}
      },
      "source": [
        "def run_multiple_examples(input_sentences, bs=32):\n",
        "    \"\"\"\n",
        "    This fucntion returns classification predictions for batch of sentences.\n",
        "    input_sentences: list of strings\n",
        "    bs : batch_size : int\n",
        "    \"\"\"\n",
        "    \n",
        "    ## Prepare data for classification\n",
        "    ids = []\n",
        "    segment_ids = []\n",
        "    input_masks = []\n",
        "    pred_lt = []\n",
        "    for sen in input_sentences:\n",
        "        text_tokens = tokenizer.tokenize(sen)\n",
        "        tokens = [\"[CLS]\"] + text_tokens + [\"[SEP]\"]\n",
        "        temp_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "        input_mask = [1] * len(temp_ids)\n",
        "        segment_id = [0] * len(temp_ids)\n",
        "        padding = [0] * (max_seq_len - len(temp_ids))\n",
        "\n",
        "        temp_ids += padding\n",
        "        input_mask += padding\n",
        "        segment_id += padding\n",
        "        \n",
        "        ids.append(temp_ids)\n",
        "        input_masks.append(input_mask)\n",
        "        segment_ids.append(segment_id)\n",
        "    \n",
        "    ## Convert input lists to Torch Tensors\n",
        "    ids = torch.tensor(ids)\n",
        "    segment_ids = torch.tensor(segment_ids)\n",
        "    input_masks = torch.tensor(input_masks)\n",
        "    \n",
        "    steps = len(ids) // bs\n",
        "    \n",
        "    for i in range(steps+1):\n",
        "        if i == steps:\n",
        "            temp_ids = ids[i * bs : len(ids)]\n",
        "            temp_segment_ids = segment_ids[i * bs: len(ids)]\n",
        "            temp_input_masks = input_masks[i * bs: len(ids)]\n",
        "        else:\n",
        "            temp_ids = ids[i * bs : i * bs + bs]\n",
        "            temp_segment_ids = segment_ids[i * bs: i * bs + bs]\n",
        "            temp_input_masks = input_masks[i * bs: i * bs + bs]\n",
        "        \n",
        "        temp_ids = temp_ids.to(device)\n",
        "        temp_segment_ids = temp_segment_ids.to(device)\n",
        "        temp_input_masks = temp_input_masks.to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            preds = sm(model_cls(temp_ids, temp_segment_ids, temp_input_masks))\n",
        "        pred_lt.extend(preds.tolist())\n",
        "    \n",
        "    return pred_lt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iphQqoHlHjr4",
        "colab": {}
      },
      "source": [
        "def read_file(path,size):\n",
        "    with open(path) as fp:\n",
        "        data = fp.read().splitlines()[:size]\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OhCa_sHuHjr5",
        "colab": {}
      },
      "source": [
        "def get_attention_for_batch(input_sentences, bs=32):\n",
        "    \"\"\"\n",
        "    This function calculates attention weights of all the heads and\n",
        "    returns it along with the encoded sentence for further processing.\n",
        "    \n",
        "    input sentence: list of strings\n",
        "    bs : batch_size\n",
        "    \"\"\"\n",
        "    \n",
        "    ## Preprocessing for BERT \n",
        "    ids = []\n",
        "    segment_ids = []\n",
        "    input_masks = []\n",
        "    pred_lt = []\n",
        "    ids_for_decoding = []\n",
        "    for sen in input_sentences:\n",
        "        text_tokens = tokenizer.tokenize(sen)\n",
        "        tokens = [\"[CLS]\"] + text_tokens[:510] + [\"[SEP]\"]\n",
        "        temp_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "        input_mask = [1] * len(temp_ids)\n",
        "        segment_id = [0] * len(temp_ids)\n",
        "        padding = [0] * (max_seq_len - len(temp_ids))\n",
        "        \n",
        "        ids_for_decoding.append(tokenizer.convert_tokens_to_ids(tokens))\n",
        "        temp_ids += padding\n",
        "        input_mask += padding\n",
        "        segment_id += padding\n",
        "        \n",
        "        ids.append(temp_ids)\n",
        "        input_masks.append(input_mask)\n",
        "        segment_ids.append(segment_id)\n",
        "    ## Convert the list of int ids to Torch Tensors\n",
        "    ids = torch.tensor(ids)\n",
        "    segment_ids = torch.tensor(segment_ids)\n",
        "    input_masks = torch.tensor(input_masks)\n",
        "    \n",
        "    steps = len(ids) // bs\n",
        "    \n",
        "    for i in trange(steps+1):\n",
        "        if i == steps:\n",
        "            temp_ids = ids[i * bs : len(ids)]\n",
        "            temp_segment_ids = segment_ids[i * bs: len(ids)]\n",
        "            temp_input_masks = input_masks[i * bs: len(ids)]\n",
        "        else:\n",
        "            temp_ids = ids[i * bs : i * bs + bs]\n",
        "            temp_segment_ids = segment_ids[i * bs: i * bs + bs]\n",
        "            temp_input_masks = input_masks[i * bs: i * bs + bs]\n",
        "        \n",
        "        temp_ids = temp_ids.to(device)\n",
        "        temp_segment_ids = temp_segment_ids.to(device)\n",
        "        temp_input_masks = temp_input_masks.to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            _, _, attn = model(temp_ids, temp_segment_ids, temp_input_masks)\n",
        "        \n",
        "        # Add all the Attention Weights to CPU memory\n",
        "        # Attention weights for each layer is stored in a dict 'attn_prob'\n",
        "        for k in range(12):\n",
        "            attn[k]['attn_probs'] = attn[k]['attn_probs'].to('cpu')\n",
        "        \n",
        "        '''\n",
        "        attention weights are stored in this way:\n",
        "        att_lt[layer]['attn_probs']['input_sentence']['head']['length_of_sentence']\n",
        "        '''\n",
        "        # Concate Attention weights for all the examples in the list att_lt[layer_no]['attn_probs']\n",
        "        \n",
        "        if i == 0:\n",
        "            att_lt = attn\n",
        "            heads = len(att_lt)\n",
        "        else:\n",
        "            for j in range(heads):\n",
        "                att_lt[j]['attn_probs'] = torch.cat((att_lt[j]['attn_probs'],attn[j]['attn_probs']),0)\n",
        "        \n",
        "    \n",
        "    return att_lt, ids_for_decoding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NR9cf9i-Hjr7",
        "colab": {}
      },
      "source": [
        "def process_sentences(input_sentences, att, decoding_ids, threshold=0.25):\n",
        "    \"\"\"\n",
        "    This function processes each input sentence by removing the top tokens defined threshold value.\n",
        "    Each sentence is processed for each head.\n",
        "    \n",
        "    input_ids: list of strings\n",
        "    decoding_ids: indexed input_sentnces thus len(input_sentences) == len(decoding_ids)\n",
        "    threshold: Percentage of the top indexes to be removed\n",
        "    \"\"\"\n",
        "    # List of None of num_of_layers * num_of_heads to save the results of each head for input_sentences\n",
        "    \n",
        "    lt = [None for x in range(len(att) * len(att[0]['attn_probs'][0]))]\n",
        "    #print(len(lt))\n",
        "    \n",
        "    inx = 0\n",
        "    for i in trange(len(att)): #  For all the layers\n",
        "        for j in range(len(att[i]['attn_probs'][0])): # For all the heads in the ith Layer\n",
        "            processed_sen = [None for q in decoding_ids] # List of len(decoding_ids)\n",
        "            for k in range(len(input_sentences)): # For all the senteces \n",
        "                _, topi = att[i]['attn_probs'][k][j][0].topk(len(decoding_ids[k])) # Get top attended ids\n",
        "                topi = topi.tolist()\n",
        "                topi = topi[:int(len(topi) * threshold)] \n",
        "                ## Decode the sentece after removing the topk indexes\n",
        "                final_indexes = []\n",
        "                count = 0\n",
        "                count1 = 0\n",
        "                tokens = [\"[CLS]\"] + tokenizer.tokenize(input_sentences[k]) + [\"[SEP]\"]\n",
        "                while count < len(decoding_ids[k]):\n",
        "                    if count in topi: # Remove index if present in topk\n",
        "                        while (count + count1 + 1) < len(decoding_ids[k]):\n",
        "                            if \"##\" in tokens[count + count1 + 1]:\n",
        "                                count1 += 1\n",
        "                            else:\n",
        "                                break\n",
        "                        count += count1\n",
        "                        count1 = 0\n",
        "                    else: # Else add to the decoded sentence\n",
        "                        final_indexes.append(decoding_ids[k][count])\n",
        "                    count += 1\n",
        "                tmp = tokenizer.convert_ids_to_tokens(final_indexes) # Convert ids to token\n",
        "                # Convert toknes to sentence\n",
        "                processed_sen[k] = \" \".join(tmp).replace(\" ##\", \"\").replace(\"[CLS]\",\"\").replace(\"[SEP]\",\"\").strip()\n",
        "            lt[inx] = processed_sen # Store sentences for inxth head\n",
        "            inx += 1\n",
        "    \n",
        "    return lt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y6F1dEo6Hjr9",
        "colab": {}
      },
      "source": [
        "def get_block_head(processed_sentence_list, lmbd = 0.1):\n",
        "    \"\"\"\n",
        "    This function calculate classification scores for sentences generated by each head\n",
        "    and sort them from best to worst.\n",
        "    score = min(pred) + lmbd / max(pred) + lmbd, lmbd is smoothing param\n",
        "    pred is list of probability score for each class, for best case pred = [0.5, 0.5] ==> score = 1\n",
        "    \n",
        "    it returns sorted list of (Layer, Head, Score)\n",
        "    \"\"\"\n",
        "    scores = {}\n",
        "    #scores_1 = {}\n",
        "    for i in trange(len(processed_sentence_list)): # sentences by each head\n",
        "        pred = np.array(run_multiple_examples(processed_sentence_list[i]))\n",
        "        scores[i] = np.mean([(min(x[0], x[1])+lmbd)/(max(x[0], x[1])+lmbd) for x in pred])\n",
        "        #scores_1[i] = np.mean([abs(max(x[0],x[1]) - min(x[0],x[1])) for x in pred])\n",
        "    temp = sorted(scores.items(), key=lambda kv: kv[1], reverse=True)\n",
        "    #temp1 = sorted(scores_1.items(), key=lambda kv: kv[1], reverse=False)\n",
        "    score_lt = [(x // 12, x - (12 * (x // 12)),y) for x,y in temp]\n",
        "    #score1_lt = [(x // 12, x - (12 * (x // 12)),y) for x,y in temp1]\n",
        "    return score_lt  #score1_lt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uBz6w944Hjr-",
        "colab": {}
      },
      "source": [
        "pos_examples_file = \"/content/style-transfer/data/dev/en.txt\"\n",
        "neg_examples_file = \"/content/style-transfer/data/dev/trump.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "24PZGgY-Hjr_",
        "colab": {}
      },
      "source": [
        "'''\n",
        "100 examples from each class worked good, the bottlenack is the run_multiple_examples() function,\n",
        "with higher memory (either with cpu of gpu) one can reduce the processing time by incresing batch_size.\n",
        "With batch_size of 32 it takes around 24 mins for 100 example on cpu.\n",
        "'''\n",
        "pos_data = read_file(pos_examples_file,100)\n",
        "neg_data = read_file(neg_examples_file,100)\n",
        "data = pos_data + neg_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EopC4ZIKHjsA",
        "outputId": "0c06b2bb-ecce-4ee9-819b-e9ae2932f99f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(pos_data), len(neg_data), len(data))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100 100 200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VEp5yoB3HjsC",
        "outputId": "442463e7-39e5-4d8c-cfd5-7c9fffdbdc9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "att, decoding_ids = get_attention_for_batch(data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 7/7 [10:12<00:00, 87.47s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3ACAarqPHjsD",
        "colab": {},
        "outputId": "b8958646-5de5-401e-e96f-9c9bbe9a9a8c"
      },
      "source": [
        "sen_list = process_sentences(data, att, decoding_ids)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:55<00:00,  4.60s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWj-WRCeZvVl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load additional module\n",
        "import pickle\n",
        "\n",
        "# define a list of places\n",
        "\n",
        "with open('senlist.data', 'wb') as f:\n",
        "    # store the data as binary data stream\n",
        "    pickle.dump(sen_list, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UYoA7NdVHjsF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8ee7ecf2-f065-497f-82cc-f426286e2cb4"
      },
      "source": [
        "# load additional module\n",
        "import pickle\n",
        "\n",
        "with open('senlist.data', 'rb') as f:\n",
        "    # read the data as binary data stream\n",
        "    sen_list = pickle.load(f)\n",
        "scores = get_block_head(sen_list)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 144/144 [09:11<00:00,  3.83s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zC8mZEv-HjsG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "67d5f806-9fbf-4c98-b610-eec7ecb612a7"
      },
      "source": [
        "scores"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 1, 0.13382814254819334),\n",
              " (10, 0, 0.1336683219685187),\n",
              " (4, 1, 0.1330806929224985),\n",
              " (4, 2, 0.1314543670159833),\n",
              " (8, 7, 0.12869235658352232),\n",
              " (10, 4, 0.12564484317694716),\n",
              " (4, 6, 0.1245197816318354),\n",
              " (10, 9, 0.12377803954796912),\n",
              " (9, 3, 0.12311589719294336),\n",
              " (9, 7, 0.12214683618135663),\n",
              " (8, 2, 0.12170949818545505),\n",
              " (10, 1, 0.12097076370518713),\n",
              " (9, 4, 0.12080693246315115),\n",
              " (6, 6, 0.12070488826954413),\n",
              " (11, 5, 0.12060356237578784),\n",
              " (9, 9, 0.1205555933646261),\n",
              " (0, 7, 0.12049272754358846),\n",
              " (11, 4, 0.1203224005820534),\n",
              " (8, 6, 0.12022378089887475),\n",
              " (9, 2, 0.1201802340811301),\n",
              " (9, 8, 0.12017242328686098),\n",
              " (10, 2, 0.11958478511851772),\n",
              " (9, 6, 0.11923838009304177),\n",
              " (2, 2, 0.11915853271870686),\n",
              " (9, 0, 0.11872476604240152),\n",
              " (10, 6, 0.11871936029934879),\n",
              " (10, 5, 0.11856610217027116),\n",
              " (3, 4, 0.11839533180812982),\n",
              " (10, 10, 0.11789990759240158),\n",
              " (8, 8, 0.11789660330693331),\n",
              " (5, 6, 0.1177994884212698),\n",
              " (7, 8, 0.11775773076614528),\n",
              " (11, 0, 0.11775418007275817),\n",
              " (8, 3, 0.11773157491485672),\n",
              " (8, 11, 0.11761491191871531),\n",
              " (8, 4, 0.11758757184457566),\n",
              " (7, 1, 0.11746219169407172),\n",
              " (11, 9, 0.11736200544052446),\n",
              " (7, 9, 0.11693126210933713),\n",
              " (7, 0, 0.11687140579275072),\n",
              " (6, 2, 0.11682836097604571),\n",
              " (8, 10, 0.11669840591410646),\n",
              " (6, 1, 0.11669084185581383),\n",
              " (8, 5, 0.1165823285012472),\n",
              " (9, 5, 0.1165809996739516),\n",
              " (9, 10, 0.11634709435452892),\n",
              " (11, 11, 0.11625149444409714),\n",
              " (11, 8, 0.11614810834873239),\n",
              " (1, 2, 0.11567148958677162),\n",
              " (6, 0, 0.11561426422013152),\n",
              " (7, 7, 0.1152258055408333),\n",
              " (2, 8, 0.11513313768280774),\n",
              " (5, 5, 0.1151096403277178),\n",
              " (6, 10, 0.11500572111292527),\n",
              " (9, 1, 0.11489912900969959),\n",
              " (7, 10, 0.11481293345729725),\n",
              " (8, 9, 0.11471400798846176),\n",
              " (2, 1, 0.11435368824935843),\n",
              " (5, 4, 0.11396725058171628),\n",
              " (5, 1, 0.11392440867086197),\n",
              " (3, 2, 0.1135759681612401),\n",
              " (6, 3, 0.11335024674397166),\n",
              " (4, 4, 0.11333297965295648),\n",
              " (2, 11, 0.11320457742805544),\n",
              " (1, 0, 0.11314267990672612),\n",
              " (8, 0, 0.11272879488835041),\n",
              " (6, 5, 0.11263373029515607),\n",
              " (10, 8, 0.11261272970920576),\n",
              " (11, 7, 0.11207212448480953),\n",
              " (4, 8, 0.11189821489242792),\n",
              " (5, 3, 0.11184951220594916),\n",
              " (5, 11, 0.1117749937545062),\n",
              " (7, 6, 0.1116695459957136),\n",
              " (6, 8, 0.1115538024783046),\n",
              " (0, 5, 0.1112912381583558),\n",
              " (6, 7, 0.11107078054912652),\n",
              " (7, 11, 0.11097910619280067),\n",
              " (7, 5, 0.11070133243561824),\n",
              " (3, 8, 0.1106307494418827),\n",
              " (1, 10, 0.1105444341387111),\n",
              " (5, 9, 0.11045321796744748),\n",
              " (3, 1, 0.11044569285372845),\n",
              " (2, 10, 0.11027896554208154),\n",
              " (1, 7, 0.11021219081973996),\n",
              " (11, 3, 0.11018686311277685),\n",
              " (5, 7, 0.11004183698268305),\n",
              " (5, 10, 0.10995867084625227),\n",
              " (10, 11, 0.10987350348008658),\n",
              " (11, 6, 0.10987263814390104),\n",
              " (2, 9, 0.1097605096716399),\n",
              " (3, 10, 0.10975652578915582),\n",
              " (1, 3, 0.10966199110897412),\n",
              " (11, 10, 0.10951688464001105),\n",
              " (4, 3, 0.10951444655793213),\n",
              " (1, 9, 0.10947355329092769),\n",
              " (7, 4, 0.10937982779074432),\n",
              " (0, 4, 0.109377704174487),\n",
              " (4, 0, 0.10935968519128732),\n",
              " (10, 3, 0.10918991054365143),\n",
              " (3, 5, 0.109150421197501),\n",
              " (2, 0, 0.10909998997515974),\n",
              " (3, 6, 0.10905333466606981),\n",
              " (5, 2, 0.10885598779274495),\n",
              " (3, 7, 0.10884827276683083),\n",
              " (6, 9, 0.10879368225204157),\n",
              " (0, 9, 0.10856628299773172),\n",
              " (2, 3, 0.10848177667337483),\n",
              " (2, 4, 0.10839912156907461),\n",
              " (10, 7, 0.10814471395040154),\n",
              " (3, 11, 0.10813924765578559),\n",
              " (4, 5, 0.10810215231417146),\n",
              " (3, 9, 0.1080772934684163),\n",
              " (2, 7, 0.10783499573947754),\n",
              " (1, 5, 0.10783221053409602),\n",
              " (3, 0, 0.10777409561432336),\n",
              " (11, 2, 0.10772293851871602),\n",
              " (5, 8, 0.10746907570005618),\n",
              " (4, 7, 0.10745124919700767),\n",
              " (6, 11, 0.10727387322007256),\n",
              " (7, 3, 0.1070741455596135),\n",
              " (11, 1, 0.10688765733362642),\n",
              " (2, 5, 0.10685948635158635),\n",
              " (7, 2, 0.10641659660139358),\n",
              " (3, 3, 0.10639329478450782),\n",
              " (8, 1, 0.1061498723052065),\n",
              " (1, 8, 0.10599486118856477),\n",
              " (1, 4, 0.10589677566381145),\n",
              " (4, 10, 0.10566750404036675),\n",
              " (4, 11, 0.10561761670613196),\n",
              " (4, 9, 0.10553932488183886),\n",
              " (5, 0, 0.10528209322239004),\n",
              " (1, 1, 0.10495076269337396),\n",
              " (9, 11, 0.10458353204138195),\n",
              " (0, 2, 0.10451354048039109),\n",
              " (2, 6, 0.10440111357505696),\n",
              " (6, 4, 0.10439135740532142),\n",
              " (1, 6, 0.10410025410370687),\n",
              " (0, 8, 0.10403165711415831),\n",
              " (0, 10, 0.10383457303564464),\n",
              " (1, 11, 0.10382083980498383),\n",
              " (0, 3, 0.10356030616838878),\n",
              " (0, 0, 0.10347524968830633),\n",
              " (0, 11, 0.10347059880193096),\n",
              " (0, 6, 0.10334478451437111)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3W_rm9kYHjsH",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}