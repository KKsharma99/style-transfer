{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "drg.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM27mmcn0wu242e5Woklv3q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tuanyuan2008/style-transfer/blob/master/drg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5ZwSo94MY1r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "c42787a2-ab49-4bb6-fe2b-a5936ac0478f"
      },
      "source": [
        "! git clone https://github.com/tuanyuan2008/style-transfer.git\n",
        "% cd style-transfer\n",
        "! git pull"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'style-transfer'...\n",
            "remote: Enumerating objects: 23, done.\u001b[K\n",
            "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 481 (delta 5), reused 19 (delta 5), pack-reused 458\u001b[K\n",
            "Receiving objects: 100% (481/481), 77.27 MiB | 8.30 MiB/s, done.\n",
            "Resolving deltas: 100% (267/267), done.\n",
            "Checking out files: 100% (178/178), done.\n",
            "/content/style-transfer\n",
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FF0cjkbqMfr8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0f2f8c75-3d11-417a-b6dc-9169d17a656d"
      },
      "source": [
        "! bash dg.sh"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
            "Namespace(do_eval=True, do_train=True, eval_batch_size=32, eval_dataset='data/processed_files/dre_model/test/all.txt', learning_rate=6.25e-05, lm_coef=0.9, lr_schedule='warmup_linear', max_grad_norm=1, max_seq_length=85, model_name='openai-gpt', n_valid=374, num_train_epochs=1, output_dir='dg_model_weights', seed=42, server_ip='', server_port='', train_batch_size=32, train_dataset='data/processed_files/dre_model/train/all.txt', warmup_proportion=0.002, weight_decay=0.01)\n",
            "04/24/2020 03:20:41 - INFO - __main__ -   device: cuda, n_gpu 1\n",
            "04/24/2020 03:20:42 - INFO - pytorch_pretrained_bert.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-vocab.json not found in cache, downloading to /tmp/tmp56jaafea\n",
            "100% 815973/815973 [00:01<00:00, 720849.88B/s]\n",
            "04/24/2020 03:20:45 - INFO - pytorch_pretrained_bert.file_utils -   copying /tmp/tmp56jaafea to cache at /root/.pytorch_pretrained_bert/4ab93d0cd78ae80e746c27c9cd34e90b470abdabe0590c9ec742df61625ba310.b9628f6fe5519626534b82ce7ec72b22ce0ae79550325f45c604a25c0ad87fd6\n",
            "04/24/2020 03:20:45 - INFO - pytorch_pretrained_bert.file_utils -   creating metadata file for /root/.pytorch_pretrained_bert/4ab93d0cd78ae80e746c27c9cd34e90b470abdabe0590c9ec742df61625ba310.b9628f6fe5519626534b82ce7ec72b22ce0ae79550325f45c604a25c0ad87fd6\n",
            "04/24/2020 03:20:45 - INFO - pytorch_pretrained_bert.file_utils -   removing temp file /tmp/tmp56jaafea\n",
            "04/24/2020 03:20:45 - INFO - pytorch_pretrained_bert.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-merges.txt not found in cache, downloading to /tmp/tmpo6nxk5ms\n",
            "100% 458495/458495 [00:01<00:00, 408813.61B/s]\n",
            "04/24/2020 03:20:48 - INFO - pytorch_pretrained_bert.file_utils -   copying /tmp/tmpo6nxk5ms to cache at /root/.pytorch_pretrained_bert/0f8de0dbd6a2bb6bde7d758f4c120dd6dd20b46f2bf0a47bc899c89f46532fde.20808570f9a3169212a577f819c845330da870aeb14c40f7319819fce10c3b76\n",
            "04/24/2020 03:20:48 - INFO - pytorch_pretrained_bert.file_utils -   creating metadata file for /root/.pytorch_pretrained_bert/0f8de0dbd6a2bb6bde7d758f4c120dd6dd20b46f2bf0a47bc899c89f46532fde.20808570f9a3169212a577f819c845330da870aeb14c40f7319819fce10c3b76\n",
            "04/24/2020 03:20:48 - INFO - pytorch_pretrained_bert.file_utils -   removing temp file /tmp/tmpo6nxk5ms\n",
            "04/24/2020 03:20:48 - INFO - pytorch_pretrained_bert.tokenization_openai -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-vocab.json from cache at /root/.pytorch_pretrained_bert/4ab93d0cd78ae80e746c27c9cd34e90b470abdabe0590c9ec742df61625ba310.b9628f6fe5519626534b82ce7ec72b22ce0ae79550325f45c604a25c0ad87fd6\n",
            "04/24/2020 03:20:48 - INFO - pytorch_pretrained_bert.tokenization_openai -   loading merges file https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-merges.txt from cache at /root/.pytorch_pretrained_bert/0f8de0dbd6a2bb6bde7d758f4c120dd6dd20b46f2bf0a47bc899c89f46532fde.20808570f9a3169212a577f819c845330da870aeb14c40f7319819fce10c3b76\n",
            "04/24/2020 03:20:48 - WARNING - pytorch_pretrained_bert.tokenization_openai -   ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n",
            "04/24/2020 03:20:48 - INFO - pytorch_pretrained_bert.tokenization_openai -   Special tokens {'<POS>': 40478, '<NEG>': 40479, '<CON_START>': 40480, '<START>': 40481, '<END>': 40482}\n",
            "04/24/2020 03:20:49 - INFO - pytorch_pretrained_bert.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-pytorch_model.bin not found in cache, downloading to /tmp/tmpc8vrk7xl\n",
            "100% 478750579/478750579 [00:41<00:00, 11563489.74B/s]\n",
            "04/24/2020 03:21:31 - INFO - pytorch_pretrained_bert.file_utils -   copying /tmp/tmpc8vrk7xl to cache at /root/.pytorch_pretrained_bert/e45ee1afb14c5d77c946e66cb0fa70073a77882097a1a2cefd51fd24b172355e.e7ee3fcd07c695a4c9f31ca735502c090230d988de03202f7af9ebe1c3a4054c\n",
            "04/24/2020 03:21:32 - INFO - pytorch_pretrained_bert.file_utils -   creating metadata file for /root/.pytorch_pretrained_bert/e45ee1afb14c5d77c946e66cb0fa70073a77882097a1a2cefd51fd24b172355e.e7ee3fcd07c695a4c9f31ca735502c090230d988de03202f7af9ebe1c3a4054c\n",
            "04/24/2020 03:21:32 - INFO - pytorch_pretrained_bert.file_utils -   removing temp file /tmp/tmpc8vrk7xl\n",
            "04/24/2020 03:21:33 - INFO - pytorch_pretrained_bert.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-config.json not found in cache, downloading to /tmp/tmp7omy9fub\n",
            "100% 326/326 [00:00<00:00, 301044.28B/s]\n",
            "04/24/2020 03:21:34 - INFO - pytorch_pretrained_bert.file_utils -   copying /tmp/tmp7omy9fub to cache at /root/.pytorch_pretrained_bert/a27bb7c70e9002d7558d2682d5a95f3c0a8b31034616309459e0b51ef07ade09.1bb6b73eca45938a31700ee9b8193eee21734d2e12bb8098bd7cf94a82070c71\n",
            "04/24/2020 03:21:34 - INFO - pytorch_pretrained_bert.file_utils -   creating metadata file for /root/.pytorch_pretrained_bert/a27bb7c70e9002d7558d2682d5a95f3c0a8b31034616309459e0b51ef07ade09.1bb6b73eca45938a31700ee9b8193eee21734d2e12bb8098bd7cf94a82070c71\n",
            "04/24/2020 03:21:34 - INFO - pytorch_pretrained_bert.file_utils -   removing temp file /tmp/tmp7omy9fub\n",
            "04/24/2020 03:21:34 - INFO - pytorch_pretrained_bert.modeling_openai -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-pytorch_model.bin from cache at /root/.pytorch_pretrained_bert/e45ee1afb14c5d77c946e66cb0fa70073a77882097a1a2cefd51fd24b172355e.e7ee3fcd07c695a4c9f31ca735502c090230d988de03202f7af9ebe1c3a4054c\n",
            "04/24/2020 03:21:34 - INFO - pytorch_pretrained_bert.modeling_openai -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-config.json from cache at /root/.pytorch_pretrained_bert/a27bb7c70e9002d7558d2682d5a95f3c0a8b31034616309459e0b51ef07ade09.1bb6b73eca45938a31700ee9b8193eee21734d2e12bb8098bd7cf94a82070c71\n",
            "04/24/2020 03:21:34 - INFO - pytorch_pretrained_bert.modeling_openai -   Model config {\n",
            "  \"afn\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"OpenAIGPTLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"n_ctx\": 512,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 512,\n",
            "  \"n_special\": 0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"vocab_size\": 40478\n",
            "}\n",
            "\n",
            "04/24/2020 03:21:47 - INFO - __main__ -   Encoding dataset...\n",
            "100% 23093/23093 [01:01<00:00, 378.28it/s]\n",
            "100% 91781/91781 [00:56<00:00, 1610.53it/s]\n",
            "Training samples = 23093\n",
            "Validation samples = 91781\n",
            "Example = [295, 1268, 255, 279, 1254, 290, 544, 556, 240, 257, 256, 481, 25969, 1541, 510, 1347, 8417, 4427, 240, 241, 16352, 668, 1320, 279, 509, 18674, 8407, 4188, 984, 240, 256, 239, 524, 240, 240, 513, 1146, 1820, 10518, 17125, 558, 895, 2737, 240, 38008, 766, 257, 512, 3814, 7379, 27615, 239, 244, 587, 239, 1056, 240, 939, 239, 244, 3351, 3633, 1122, 5560, 19299, 1256, 9321, 240, 260, 260, 279, 24430, 509, 11783, 599, 240, 239, 487, 240, 1964, 10640, 844, 19593, 599, 2830, 33083, 617, 11997, 4390, 9400, 2217, 240, 239, 999, 4258, 239, 481, 7180, 2080, 500, 3105, 13573, 239, 4360, 239, 19449, 240, 1412, 500, 38858, 3159, 12418, 239, 487, 783, 868, 589, 5918, 239, 244, 481, 270, 1741, 40480, 249, 603, 249, 636, 645, 487, 866, 507, 3638, 485, 604, 566, 239, 244, 3351, 983, 547, 1662, 239, 244, 481, 1164, 14528, 11072, 485, 531, 32316, 504, 481, 728, 737, 498, 8, 250, 239, 487, 759, 925, 512, 1129, 4273, 239, 524, 21807, 641, 3140, 1147, 1036, 32670, 488, 600, 641, 23500, 1147, 1036, 32528, 239, 999, 737, 498, 40481, 249, 603, 249, 636, 240, 645, 487, 866, 507, 3638, 485, 604, 566, 239, 244, 3351, 983, 240, 547, 1662, 239, 244, 481, 1164, 14528, 11072, 485, 531, 32316, 504, 481, 728, 737, 498, 8, 250, 239, 487, 759, 925, 512, 1129, 4273, 239, 524, 21807, 641, 3140, 1147, 1036, 32670, 240, 488, 600, 641, 23500, 1147, 1036, 32528, 239, 999, 737, 498, 19593, 260, 260, 1347, 279, 24430, 1320, 279, 260, 260, 759, 256, 241, 825, 895, 240, 1122, 5362, 240, 249, 868, 17808, 507, 512, 781, 239, 244, 587, 8407, 2425, 13573, 257, 256, 481, 18674, 25969, 19449, 27968, 485, 33083, 7180, 5560, 544, 8, 250, 239, 487, 509, 1359, 2484, 988, 481, 5608, 498, 246, 4390, 617, 9400, 240, 649, 481, 27615, 3633, 240, 488, 487, 509, 668, 670, 485, 1964, 1146, 669, 606, 589, 3159, 666, 481, 844, 239, 4360, 239, 11783, 509, 3105, 500, 783, 4188, 240, 488, 939, 485, 16352, 500, 3814, 7379, 239, 1056, 481, 3334, 2830, 246, 1741, 1685, 2737, 257, 512, 823, 599, 664, 16718, 871, 2443, 239, 244, 481, 1541, 9322, 1412, 1907, 510, 240, 488, 487, 509, 18080, 556, 599, 487, 558, 1256, 239, 481, 728, 509, 556, 513, 4427, 2556, 488, 754, 19299, 11997, 2080, 239, 481, 1164, 10518, 240, 2217, 240, 984, 17125, 8417, 239, 12521, 240, 4258, 481, 10640, 498, 38858, 240, 38008, 766, 488, 1820, 240, 9321, 488, 12418, 240, 488, 5918, 30002, 270, 270, 40482]\n",
            "Input Length = 85\n",
            "Training Example Input ids= tensor([  295,  1268,   255,   279,  1254,   290,  8954, 40480, 40481,  8954,\n",
            "        40482,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0])\n",
            "Training Example Language Modeling ids = tensor([   -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,  8954, 40482,\n",
            "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
            "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
            "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
            "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
            "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
            "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
            "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
            "           -1,    -1,    -1,    -1,    -1])\n",
            "Epoch:   0% 0/1 [00:00<?, ?it/s]\n",
            "Training:   0% 0/182 [00:00<?, ?it/s]\u001b[A\n",
            "Training loss: 2.74e+00 lr: 6.22e-05:   1% 1/182 [00:01<03:03,  1.01s/it]\u001b[A\n",
            "Training loss: 2.58e+00 lr: 6.18e-05:   1% 2/182 [00:01<02:47,  1.07it/s]\u001b[A\n",
            "Training loss: 2.49e+00 lr: 6.15e-05:   2% 3/182 [00:02<02:36,  1.15it/s]\u001b[A\n",
            "Training loss: 2.36e+00 lr: 6.11e-05:   2% 4/182 [00:03<02:28,  1.20it/s]\u001b[A\n",
            "Training loss: 2.21e+00 lr: 6.08e-05:   3% 5/182 [00:03<02:22,  1.24it/s]\u001b[A\n",
            "Training loss: 2.10e+00 lr: 6.04e-05:   3% 6/182 [00:04<02:18,  1.27it/s]\u001b[A\n",
            "Training loss: 2.03e+00 lr: 6.01e-05:   4% 7/182 [00:05<02:16,  1.28it/s]\u001b[A\n",
            "Training loss: 1.88e+00 lr: 5.97e-05:   4% 8/182 [00:06<02:13,  1.30it/s]\u001b[A\n",
            "Training loss: 1.85e+00 lr: 5.94e-05:   5% 9/182 [00:06<02:12,  1.31it/s]\u001b[A\n",
            "Training loss: 1.75e+00 lr: 5.90e-05:   5% 10/182 [00:07<02:10,  1.32it/s]\u001b[A\n",
            "Training loss: 1.67e+00 lr: 5.87e-05:   6% 11/182 [00:08<02:08,  1.33it/s]\u001b[A\n",
            "Training loss: 1.59e+00 lr: 5.84e-05:   7% 12/182 [00:09<02:07,  1.33it/s]\u001b[A\n",
            "Training loss: 1.54e+00 lr: 5.80e-05:   7% 13/182 [00:09<02:06,  1.33it/s]\u001b[A\n",
            "Training loss: 1.49e+00 lr: 5.77e-05:   8% 14/182 [00:10<02:06,  1.33it/s]\u001b[A\n",
            "Training loss: 1.40e+00 lr: 5.73e-05:   8% 15/182 [00:11<02:05,  1.33it/s]\u001b[A\n",
            "Training loss: 1.39e+00 lr: 5.70e-05:   9% 16/182 [00:12<02:04,  1.34it/s]\u001b[A\n",
            "Training loss: 1.40e+00 lr: 5.66e-05:   9% 17/182 [00:12<02:04,  1.33it/s]\u001b[A\n",
            "Training loss: 1.36e+00 lr: 5.63e-05:  10% 18/182 [00:13<02:03,  1.32it/s]\u001b[A\n",
            "Training loss: 1.35e+00 lr: 5.59e-05:  10% 19/182 [00:14<02:03,  1.32it/s]\u001b[A\n",
            "Training loss: 1.29e+00 lr: 5.56e-05:  11% 20/182 [00:15<02:03,  1.31it/s]\u001b[A\n",
            "Training loss: 1.26e+00 lr: 5.52e-05:  12% 21/182 [00:16<02:03,  1.31it/s]\u001b[A\n",
            "Training loss: 1.25e+00 lr: 5.49e-05:  12% 22/182 [00:16<02:02,  1.31it/s]\u001b[A\n",
            "Training loss: 1.22e+00 lr: 5.46e-05:  13% 23/182 [00:17<02:01,  1.31it/s]\u001b[A\n",
            "Training loss: 1.18e+00 lr: 5.42e-05:  13% 24/182 [00:18<02:01,  1.30it/s]\u001b[A\n",
            "Training loss: 1.17e+00 lr: 5.39e-05:  14% 25/182 [00:19<02:00,  1.30it/s]\u001b[A\n",
            "Training loss: 1.12e+00 lr: 5.35e-05:  14% 26/182 [00:19<02:00,  1.29it/s]\u001b[A\n",
            "Training loss: 1.11e+00 lr: 5.32e-05:  15% 27/182 [00:20<01:59,  1.29it/s]\u001b[A\n",
            "Training loss: 1.07e+00 lr: 5.28e-05:  15% 28/182 [00:21<01:59,  1.29it/s]\u001b[A\n",
            "Training loss: 1.10e+00 lr: 5.25e-05:  16% 29/182 [00:22<01:58,  1.29it/s]\u001b[A\n",
            "Training loss: 1.14e+00 lr: 5.21e-05:  16% 30/182 [00:23<01:58,  1.28it/s]\u001b[A\n",
            "Training loss: 1.12e+00 lr: 5.18e-05:  17% 31/182 [00:23<01:57,  1.28it/s]\u001b[A\n",
            "Training loss: 1.11e+00 lr: 5.15e-05:  18% 32/182 [00:24<01:57,  1.28it/s]\u001b[A\n",
            "Training loss: 1.06e+00 lr: 5.11e-05:  18% 33/182 [00:25<01:56,  1.28it/s]\u001b[A\n",
            "Training loss: 1.06e+00 lr: 5.08e-05:  19% 34/182 [00:26<01:56,  1.27it/s]\u001b[A\n",
            "Training loss: 1.05e+00 lr: 5.04e-05:  19% 35/182 [00:26<01:56,  1.26it/s]\u001b[A\n",
            "Training loss: 1.02e+00 lr: 5.01e-05:  20% 36/182 [00:27<01:55,  1.26it/s]\u001b[A\n",
            "Training loss: 1.05e+00 lr: 4.97e-05:  20% 37/182 [00:28<01:55,  1.26it/s]\u001b[A\n",
            "Training loss: 1.05e+00 lr: 4.94e-05:  21% 38/182 [00:29<01:54,  1.25it/s]\u001b[A\n",
            "Training loss: 1.04e+00 lr: 4.90e-05:  21% 39/182 [00:30<01:54,  1.25it/s]\u001b[A\n",
            "Training loss: 1.04e+00 lr: 4.87e-05:  22% 40/182 [00:30<01:53,  1.25it/s]\u001b[A\n",
            "Training loss: 1.01e+00 lr: 4.83e-05:  23% 41/182 [00:31<01:53,  1.24it/s]\u001b[A\n",
            "Training loss: 1.03e+00 lr: 4.80e-05:  23% 42/182 [00:32<01:52,  1.24it/s]\u001b[A\n",
            "Training loss: 1.03e+00 lr: 4.77e-05:  24% 43/182 [00:33<01:52,  1.24it/s]\u001b[A\n",
            "Training loss: 9.82e-01 lr: 4.73e-05:  24% 44/182 [00:34<01:51,  1.23it/s]\u001b[A\n",
            "Training loss: 9.49e-01 lr: 4.70e-05:  25% 45/182 [00:35<01:51,  1.23it/s]\u001b[A\n",
            "Training loss: 9.49e-01 lr: 4.66e-05:  25% 46/182 [00:35<01:50,  1.23it/s]\u001b[A\n",
            "Training loss: 9.28e-01 lr: 4.63e-05:  26% 47/182 [00:36<01:49,  1.23it/s]\u001b[A\n",
            "Training loss: 9.03e-01 lr: 4.59e-05:  26% 48/182 [00:37<01:48,  1.23it/s]\u001b[A\n",
            "Training loss: 9.05e-01 lr: 4.56e-05:  27% 49/182 [00:38<01:47,  1.23it/s]\u001b[A\n",
            "Training loss: 9.22e-01 lr: 4.52e-05:  27% 50/182 [00:39<01:47,  1.23it/s]\u001b[A\n",
            "Training loss: 9.11e-01 lr: 4.49e-05:  28% 51/182 [00:39<01:46,  1.23it/s]\u001b[A\n",
            "Training loss: 8.88e-01 lr: 4.45e-05:  29% 52/182 [00:40<01:45,  1.23it/s]\u001b[A\n",
            "Training loss: 9.05e-01 lr: 4.42e-05:  29% 53/182 [00:41<01:44,  1.23it/s]\u001b[A\n",
            "Training loss: 8.70e-01 lr: 4.39e-05:  30% 54/182 [00:42<01:43,  1.24it/s]\u001b[A\n",
            "Training loss: 8.48e-01 lr: 4.35e-05:  30% 55/182 [00:43<01:42,  1.24it/s]\u001b[A\n",
            "Training loss: 8.74e-01 lr: 4.32e-05:  31% 56/182 [00:43<01:41,  1.24it/s]\u001b[A\n",
            "Training loss: 8.64e-01 lr: 4.28e-05:  31% 57/182 [00:44<01:40,  1.24it/s]\u001b[A\n",
            "Training loss: 8.41e-01 lr: 4.25e-05:  32% 58/182 [00:45<01:39,  1.24it/s]\u001b[A\n",
            "Training loss: 8.39e-01 lr: 4.21e-05:  32% 59/182 [00:46<01:38,  1.25it/s]\u001b[A\n",
            "Training loss: 8.93e-01 lr: 4.18e-05:  33% 60/182 [00:47<01:37,  1.25it/s]\u001b[A\n",
            "Training loss: 9.03e-01 lr: 4.14e-05:  34% 61/182 [00:47<01:36,  1.25it/s]\u001b[A\n",
            "Training loss: 8.97e-01 lr: 4.11e-05:  34% 62/182 [00:48<01:36,  1.25it/s]\u001b[A\n",
            "Training loss: 8.90e-01 lr: 4.07e-05:  35% 63/182 [00:49<01:34,  1.26it/s]\u001b[A\n",
            "Training loss: 9.17e-01 lr: 4.04e-05:  35% 64/182 [00:50<01:33,  1.26it/s]\u001b[A\n",
            "Training loss: 8.56e-01 lr: 4.01e-05:  36% 65/182 [00:51<01:32,  1.26it/s]\u001b[A\n",
            "Training loss: 8.34e-01 lr: 3.97e-05:  36% 66/182 [00:51<01:31,  1.26it/s]\u001b[A\n",
            "Training loss: 8.50e-01 lr: 3.94e-05:  37% 67/182 [00:52<01:30,  1.27it/s]\u001b[A\n",
            "Training loss: 8.68e-01 lr: 3.90e-05:  37% 68/182 [00:53<01:29,  1.27it/s]\u001b[A\n",
            "Training loss: 8.40e-01 lr: 3.87e-05:  38% 69/182 [00:54<01:28,  1.27it/s]\u001b[A\n",
            "Training loss: 8.08e-01 lr: 3.83e-05:  38% 70/182 [00:55<01:28,  1.27it/s]\u001b[A\n",
            "Training loss: 8.41e-01 lr: 3.80e-05:  39% 71/182 [00:55<01:27,  1.28it/s]\u001b[A\n",
            "Training loss: 8.42e-01 lr: 3.76e-05:  40% 72/182 [00:56<01:25,  1.28it/s]\u001b[A\n",
            "Training loss: 8.31e-01 lr: 3.73e-05:  40% 73/182 [00:57<01:25,  1.28it/s]\u001b[A\n",
            "Training loss: 8.21e-01 lr: 3.69e-05:  41% 74/182 [00:58<01:24,  1.28it/s]\u001b[A\n",
            "Training loss: 8.54e-01 lr: 3.66e-05:  41% 75/182 [00:58<01:23,  1.29it/s]\u001b[A\n",
            "Training loss: 8.03e-01 lr: 3.63e-05:  42% 76/182 [00:59<01:22,  1.29it/s]\u001b[A\n",
            "Training loss: 7.57e-01 lr: 3.59e-05:  42% 77/182 [01:00<01:21,  1.29it/s]\u001b[A\n",
            "Training loss: 7.45e-01 lr: 3.56e-05:  43% 78/182 [01:01<01:20,  1.29it/s]\u001b[A\n",
            "Training loss: 7.67e-01 lr: 3.52e-05:  43% 79/182 [01:02<01:19,  1.29it/s]\u001b[A\n",
            "Training loss: 7.98e-01 lr: 3.49e-05:  44% 80/182 [01:02<01:18,  1.30it/s]\u001b[A\n",
            "Training loss: 7.98e-01 lr: 3.45e-05:  45% 81/182 [01:03<01:17,  1.30it/s]\u001b[A\n",
            "Training loss: 7.91e-01 lr: 3.42e-05:  45% 82/182 [01:04<01:16,  1.30it/s]\u001b[A\n",
            "Training loss: 7.52e-01 lr: 3.38e-05:  46% 83/182 [01:05<01:15,  1.30it/s]\u001b[A\n",
            "Training loss: 7.20e-01 lr: 3.35e-05:  46% 84/182 [01:05<01:15,  1.31it/s]\u001b[A\n",
            "Training loss: 7.69e-01 lr: 3.31e-05:  47% 85/182 [01:06<01:14,  1.31it/s]\u001b[A\n",
            "Training loss: 7.66e-01 lr: 3.28e-05:  47% 86/182 [01:07<01:13,  1.30it/s]\u001b[A\n",
            "Training loss: 7.40e-01 lr: 3.25e-05:  48% 87/182 [01:08<01:12,  1.31it/s]\u001b[A\n",
            "Training loss: 7.42e-01 lr: 3.21e-05:  48% 88/182 [01:08<01:11,  1.31it/s]\u001b[A\n",
            "Training loss: 7.06e-01 lr: 3.18e-05:  49% 89/182 [01:09<01:11,  1.31it/s]\u001b[A\n",
            "Training loss: 7.52e-01 lr: 3.14e-05:  49% 90/182 [01:10<01:10,  1.31it/s]\u001b[A\n",
            "Training loss: 7.85e-01 lr: 3.11e-05:  50% 91/182 [01:11<01:09,  1.31it/s]\u001b[A\n",
            "Training loss: 8.21e-01 lr: 3.07e-05:  51% 92/182 [01:11<01:08,  1.31it/s]\u001b[A\n",
            "Training loss: 8.06e-01 lr: 3.04e-05:  51% 93/182 [01:12<01:07,  1.31it/s]\u001b[A\n",
            "Training loss: 7.86e-01 lr: 3.00e-05:  52% 94/182 [01:13<01:07,  1.31it/s]\u001b[A\n",
            "Training loss: 7.72e-01 lr: 2.97e-05:  52% 95/182 [01:14<01:06,  1.31it/s]\u001b[A\n",
            "Training loss: 7.78e-01 lr: 2.94e-05:  53% 96/182 [01:14<01:05,  1.31it/s]\u001b[A\n",
            "Training loss: 7.59e-01 lr: 2.90e-05:  53% 97/182 [01:15<01:04,  1.31it/s]\u001b[A\n",
            "Training loss: 7.69e-01 lr: 2.87e-05:  54% 98/182 [01:16<01:03,  1.32it/s]\u001b[A\n",
            "Training loss: 7.78e-01 lr: 2.83e-05:  54% 99/182 [01:17<01:02,  1.32it/s]\u001b[A\n",
            "Training loss: 7.07e-01 lr: 2.80e-05:  55% 100/182 [01:18<01:02,  1.31it/s]\u001b[A\n",
            "Training loss: 7.29e-01 lr: 2.76e-05:  55% 101/182 [01:18<01:01,  1.32it/s]\u001b[A\n",
            "Training loss: 6.96e-01 lr: 2.73e-05:  56% 102/182 [01:19<01:00,  1.32it/s]\u001b[A\n",
            "Training loss: 7.22e-01 lr: 2.69e-05:  57% 103/182 [01:20<00:59,  1.32it/s]\u001b[A\n",
            "Training loss: 6.79e-01 lr: 2.66e-05:  57% 104/182 [01:21<00:59,  1.32it/s]\u001b[A\n",
            "Training loss: 7.26e-01 lr: 2.62e-05:  58% 105/182 [01:21<00:58,  1.32it/s]\u001b[A\n",
            "Training loss: 6.95e-01 lr: 2.59e-05:  58% 106/182 [01:22<00:57,  1.32it/s]\u001b[A\n",
            "Training loss: 7.02e-01 lr: 2.56e-05:  59% 107/182 [01:23<00:56,  1.32it/s]\u001b[A\n",
            "Training loss: 7.58e-01 lr: 2.52e-05:  59% 108/182 [01:24<00:56,  1.32it/s]\u001b[A\n",
            "Training loss: 7.28e-01 lr: 2.49e-05:  60% 109/182 [01:24<00:55,  1.31it/s]\u001b[A\n",
            "Training loss: 7.21e-01 lr: 2.45e-05:  60% 110/182 [01:25<00:54,  1.31it/s]\u001b[A\n",
            "Training loss: 7.26e-01 lr: 2.42e-05:  61% 111/182 [01:26<00:54,  1.31it/s]\u001b[A\n",
            "Training loss: 7.65e-01 lr: 2.38e-05:  62% 112/182 [01:27<00:53,  1.31it/s]\u001b[A\n",
            "Training loss: 7.25e-01 lr: 2.35e-05:  62% 113/182 [01:27<00:52,  1.31it/s]\u001b[A\n",
            "Training loss: 7.06e-01 lr: 2.31e-05:  63% 114/182 [01:28<00:51,  1.31it/s]\u001b[A\n",
            "Training loss: 6.90e-01 lr: 2.28e-05:  63% 115/182 [01:29<00:51,  1.31it/s]\u001b[A\n",
            "Training loss: 7.39e-01 lr: 2.24e-05:  64% 116/182 [01:30<00:50,  1.32it/s]\u001b[A\n",
            "Training loss: 7.52e-01 lr: 2.21e-05:  64% 117/182 [01:30<00:49,  1.31it/s]\u001b[A\n",
            "Training loss: 7.43e-01 lr: 2.18e-05:  65% 118/182 [01:31<00:48,  1.31it/s]\u001b[A\n",
            "Training loss: 7.63e-01 lr: 2.14e-05:  65% 119/182 [01:32<00:48,  1.31it/s]\u001b[A\n",
            "Training loss: 7.81e-01 lr: 2.11e-05:  66% 120/182 [01:33<00:47,  1.31it/s]\u001b[A\n",
            "Training loss: 7.12e-01 lr: 2.07e-05:  66% 121/182 [01:34<00:46,  1.31it/s]\u001b[A\n",
            "Training loss: 7.08e-01 lr: 2.04e-05:  67% 122/182 [01:34<00:46,  1.30it/s]\u001b[A\n",
            "Training loss: 7.01e-01 lr: 2.00e-05:  68% 123/182 [01:35<00:45,  1.30it/s]\u001b[A\n",
            "Training loss: 6.99e-01 lr: 1.97e-05:  68% 124/182 [01:36<00:44,  1.30it/s]\u001b[A\n",
            "Training loss: 7.11e-01 lr: 1.93e-05:  69% 125/182 [01:37<00:43,  1.30it/s]\u001b[A\n",
            "Training loss: 6.87e-01 lr: 1.90e-05:  69% 126/182 [01:37<00:43,  1.30it/s]\u001b[A\n",
            "Training loss: 6.65e-01 lr: 1.86e-05:  70% 127/182 [01:38<00:42,  1.30it/s]\u001b[A\n",
            "Training loss: 6.62e-01 lr: 1.83e-05:  70% 128/182 [01:39<00:41,  1.30it/s]\u001b[A\n",
            "Training loss: 6.41e-01 lr: 1.80e-05:  71% 129/182 [01:40<00:40,  1.30it/s]\u001b[A\n",
            "Training loss: 6.71e-01 lr: 1.76e-05:  71% 130/182 [01:40<00:40,  1.30it/s]\u001b[A\n",
            "Training loss: 6.53e-01 lr: 1.73e-05:  72% 131/182 [01:41<00:39,  1.30it/s]\u001b[A\n",
            "Training loss: 6.61e-01 lr: 1.69e-05:  73% 132/182 [01:42<00:38,  1.30it/s]\u001b[A\n",
            "Training loss: 6.57e-01 lr: 1.66e-05:  73% 133/182 [01:43<00:37,  1.30it/s]\u001b[A\n",
            "Training loss: 6.32e-01 lr: 1.62e-05:  74% 134/182 [01:44<00:36,  1.30it/s]\u001b[A\n",
            "Training loss: 6.78e-01 lr: 1.59e-05:  74% 135/182 [01:44<00:36,  1.30it/s]\u001b[A\n",
            "Training loss: 6.70e-01 lr: 1.55e-05:  75% 136/182 [01:45<00:35,  1.30it/s]\u001b[A\n",
            "Training loss: 6.91e-01 lr: 1.52e-05:  75% 137/182 [01:46<00:34,  1.29it/s]\u001b[A\n",
            "Training loss: 6.99e-01 lr: 1.48e-05:  76% 138/182 [01:47<00:34,  1.29it/s]\u001b[A\n",
            "Training loss: 7.05e-01 lr: 1.45e-05:  76% 139/182 [01:47<00:33,  1.29it/s]\u001b[A\n",
            "Training loss: 6.76e-01 lr: 1.42e-05:  77% 140/182 [01:48<00:32,  1.29it/s]\u001b[A\n",
            "Training loss: 6.40e-01 lr: 1.38e-05:  77% 141/182 [01:49<00:31,  1.29it/s]\u001b[A\n",
            "Training loss: 6.60e-01 lr: 1.35e-05:  78% 142/182 [01:50<00:31,  1.29it/s]\u001b[A\n",
            "Training loss: 6.72e-01 lr: 1.31e-05:  79% 143/182 [01:50<00:30,  1.29it/s]\u001b[A\n",
            "Training loss: 6.88e-01 lr: 1.28e-05:  79% 144/182 [01:51<00:29,  1.29it/s]\u001b[A\n",
            "Training loss: 7.20e-01 lr: 1.24e-05:  80% 145/182 [01:52<00:28,  1.29it/s]\u001b[A\n",
            "Training loss: 7.12e-01 lr: 1.21e-05:  80% 146/182 [01:53<00:28,  1.28it/s]\u001b[A\n",
            "Training loss: 7.00e-01 lr: 1.17e-05:  81% 147/182 [01:54<00:27,  1.28it/s]\u001b[A\n",
            "Training loss: 6.61e-01 lr: 1.14e-05:  81% 148/182 [01:54<00:26,  1.28it/s]\u001b[A\n",
            "Training loss: 6.82e-01 lr: 1.10e-05:  82% 149/182 [01:55<00:25,  1.28it/s]\u001b[A\n",
            "Training loss: 6.55e-01 lr: 1.07e-05:  82% 150/182 [01:56<00:25,  1.28it/s]\u001b[A\n",
            "Training loss: 6.35e-01 lr: 1.04e-05:  83% 151/182 [01:57<00:24,  1.28it/s]\u001b[A\n",
            "Training loss: 6.59e-01 lr: 1.00e-05:  84% 152/182 [01:58<00:23,  1.28it/s]\u001b[A\n",
            "Training loss: 6.34e-01 lr: 9.67e-06:  84% 153/182 [01:58<00:22,  1.28it/s]\u001b[A\n",
            "Training loss: 6.35e-01 lr: 9.32e-06:  85% 154/182 [01:59<00:21,  1.28it/s]\u001b[A\n",
            "Training loss: 6.60e-01 lr: 8.98e-06:  85% 155/182 [02:00<00:21,  1.28it/s]\u001b[A\n",
            "Training loss: 6.33e-01 lr: 8.63e-06:  86% 156/182 [02:01<00:20,  1.27it/s]\u001b[A\n",
            "Training loss: 6.76e-01 lr: 8.29e-06:  86% 157/182 [02:01<00:19,  1.27it/s]\u001b[A\n",
            "Training loss: 6.60e-01 lr: 7.94e-06:  87% 158/182 [02:02<00:18,  1.27it/s]\u001b[A\n",
            "Training loss: 6.48e-01 lr: 7.60e-06:  87% 159/182 [02:03<00:18,  1.27it/s]\u001b[A\n",
            "Training loss: 6.29e-01 lr: 7.25e-06:  88% 160/182 [02:04<00:17,  1.27it/s]\u001b[A\n",
            "Training loss: 6.53e-01 lr: 6.91e-06:  88% 161/182 [02:05<00:16,  1.27it/s]\u001b[A\n",
            "Training loss: 6.36e-01 lr: 6.56e-06:  89% 162/182 [02:05<00:15,  1.28it/s]\u001b[A\n",
            "Training loss: 6.58e-01 lr: 6.22e-06:  90% 163/182 [02:06<00:14,  1.28it/s]\u001b[A\n",
            "Training loss: 6.61e-01 lr: 5.87e-06:  90% 164/182 [02:07<00:14,  1.28it/s]\u001b[A\n",
            "Training loss: 6.77e-01 lr: 5.52e-06:  91% 165/182 [02:08<00:13,  1.28it/s]\u001b[A\n",
            "Training loss: 6.66e-01 lr: 5.18e-06:  91% 166/182 [02:09<00:12,  1.27it/s]\u001b[A\n",
            "Training loss: 6.70e-01 lr: 4.83e-06:  92% 167/182 [02:09<00:11,  1.28it/s]\u001b[A\n",
            "Training loss: 6.90e-01 lr: 4.49e-06:  92% 168/182 [02:10<00:10,  1.27it/s]\u001b[A\n",
            "Training loss: 6.75e-01 lr: 4.14e-06:  93% 169/182 [02:11<00:10,  1.28it/s]\u001b[A\n",
            "Training loss: 6.68e-01 lr: 3.80e-06:  93% 170/182 [02:12<00:09,  1.28it/s]\u001b[A\n",
            "Training loss: 6.69e-01 lr: 3.45e-06:  94% 171/182 [02:12<00:08,  1.28it/s]\u001b[A\n",
            "Training loss: 6.77e-01 lr: 3.11e-06:  95% 172/182 [02:13<00:07,  1.28it/s]\u001b[A\n",
            "Training loss: 6.93e-01 lr: 2.76e-06:  95% 173/182 [02:14<00:07,  1.27it/s]\u001b[A\n",
            "Training loss: 6.44e-01 lr: 2.42e-06:  96% 174/182 [02:15<00:06,  1.28it/s]\u001b[A\n",
            "Training loss: 6.49e-01 lr: 2.07e-06:  96% 175/182 [02:16<00:05,  1.28it/s]\u001b[A\n",
            "Training loss: 6.22e-01 lr: 1.73e-06:  97% 176/182 [02:16<00:04,  1.29it/s]\u001b[A\n",
            "Training loss: 6.60e-01 lr: 1.38e-06:  97% 177/182 [02:17<00:03,  1.28it/s]\u001b[A\n",
            "Training loss: 6.45e-01 lr: 1.04e-06:  98% 178/182 [02:18<00:03,  1.29it/s]\u001b[A\n",
            "Training loss: 6.38e-01 lr: 6.91e-07:  98% 179/182 [02:19<00:02,  1.29it/s]\u001b[A\n",
            "Training loss: 6.38e-01 lr: 3.45e-07:  99% 180/182 [02:19<00:01,  1.29it/s]\u001b[A\n",
            "Training loss: 6.16e-01 lr: 0.00e+00:  99% 181/182 [02:20<00:00,  1.29it/s]\u001b[A\n",
            "Training loss: 6.08e-01 lr: -3.45e-07: 100% 182/182 [02:21<00:00,  1.29it/s]\n",
            "Epoch: 100% 1/1 [02:23<00:00, 143.81s/it]\n",
            "Evaluating: 100% 2380/2380 [10:41<00:00,  3.71it/s]\n",
            "04/24/2020 03:37:02 - INFO - __main__ -   ***** Eval results *****\n",
            "04/24/2020 03:37:02 - INFO - __main__ -     eval_loss = 0.6815467076141293\n",
            "04/24/2020 03:37:02 - INFO - __main__ -     train_loss = 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR04ZjPAQ52P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "54a49843-3d9f-4754-8b46-fbbe5aa1ad20"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "! zip -r /content/file.zip /content/style-transfer/dg_model_weights/\n",
        "files.download(\"/content/file.zip\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: content/style-transfer/dg_model_weights/ (stored 0%)\n",
            "  adding: content/style-transfer/dg_model_weights/eval_results.txt (deflated 13%)\n",
            "  adding: content/style-transfer/dg_model_weights/pytorch_model_zero_grad_1.bin (deflated 10%)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-04c2735ceb21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' zip -r /content/file.zip /content/style-transfer/dg_model_weights/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/file.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0;34m'port'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m       \u001b[0;34m'path'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m       \u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m   })\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: TypeError: Could not connect to the server."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yr-pSu4vMiIq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! bash drg.sh"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}